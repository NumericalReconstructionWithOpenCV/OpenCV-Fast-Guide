{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " python: 2.7.10 (default, Feb  6 2017, 23:53:20) \n",
      "[GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.34)]\n",
      "opencv: 3.2.0\n",
      "pytesseract: pytesseract version 0.1.6\n"
     ]
    }
   ],
   "source": [
    "# OpenCV 및 OCR모듈 설치 유무 확인\n",
    "\n",
    "import sys\n",
    "import cv2\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "\n",
    "# 윈도우에서 주석해제 (tesseract_path: tesseract설치경로 확인 후 붙여넣기)\n",
    "# tesseract_path = 'C:/Program Files (x86)/Tesseract-OCR'\n",
    "# pytesseract.pytesseract.tesseract_cmd = tesseract_path + '/tesseract'\n",
    "\n",
    "print \"python:\", sys.version\n",
    "print \"opencv:\", cv2.__version__\n",
    "print \"pytesseract:\", pytesseract.image_to_string(Image.open(\"images/test.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# OpenCV - 이미지 읽기, 쓰기 및 표시하기 (1)\n",
    "\n",
    "import cv2\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def handle_image():\n",
    "    imgfile = 'images/sample.png'\n",
    "    img = cv2.imread(imgfile, cv2.IMREAD_COLOR)\n",
    "    \n",
    "    #cv2.imshow('image', img)\n",
    "    \n",
    "    #cv2.waitKey(0)\n",
    "    #cv2.destroyAllWindows()\n",
    "    #cv2.waitKey(1)\n",
    "    plt.imshow(img)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    handle_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# OpenCV - 이미지 읽기, 쓰기 및 표시하기 (2)\n",
    "\n",
    "import cv2\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def handle_image():\n",
    "    imgfile = 'images/sample.png'\n",
    "    img = cv2.imread(imgfile, cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    plt.imshow(img, cmap = \"gray\")\n",
    "    \n",
    "    #cv2.namedWindow('image', cv2.WINDOW_NORMAL)\n",
    "    #cv2.imshow('image', img)\n",
    "    #k = cv2.waitKey(0)\n",
    "    # wait for ESC key to exit\n",
    "    #if k == 27:\n",
    "        #cv2.destroyAllWindows()\n",
    "        #cv2.waitKey(1)\n",
    "    # wait for 's' key to save and exit\n",
    "    #elif k == ord('s'):\n",
    "    #    cv2.imwrite('grayImage.png', img)\n",
    "    #    cv2.destroyAllWindows()\n",
    "    #    cv2.waitKey(1)\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    handle_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# OpenCV - 도형 외곽 추출하기 (1)\n",
    "\n",
    "import cv2\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def contour():\n",
    "    imgfile = 'images/contour.jpg'\n",
    "    img = cv2.imread(imgfile)\n",
    "    imgray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    edge = cv2.Canny(imgray, 100, 200)\n",
    "    _, contours, hierarchy = cv2.findContours(edge, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)#hierarchy\n",
    "    \n",
    "    #cv2.imshow('edge', edge)\n",
    "    #plt.imshow(edge, cmap = \"gray\")\n",
    "    cv2.drawContours(img, contours, -1, (0, 255, 0), 1)\n",
    "    #cv2.imshow('Contour', img)\n",
    "    plt.imshow(img, cmap = \"gray\")\n",
    "    \n",
    "    #cv2.waitKey(0)\n",
    "    #cv2.destroyAllWindows()\n",
    "    #cv2.waitKey(1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    contour() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# OpenCV - 도형 외곽 추출하기 (2)\n",
    "\n",
    "import cv2\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def contour_approx():\n",
    "    imgfile = 'images/contour2.png'\n",
    "    img = cv2.imread(imgfile)\n",
    "    img2 = img.copy()\n",
    "    imgray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    edge = cv2.Canny(imgray, 100, 200)\n",
    "    _, contours, hierarchy = cv2.findContours(edge, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    cnt = contours[0]\n",
    "    cv2.drawContours(img, [cnt], 0, (0, 255, 0), 3)\n",
    "    \n",
    "    epsilon = 0.1 * cv2.arcLength(cnt, True)#10% 오차율(2~5%가 적당)\n",
    "    \n",
    "    approx = cv2.approxPolyDP(cnt, epsilon, True)#꼭지점을 입실론 베이스로 줄여나감\n",
    "    \n",
    "    cv2.drawContours(img2, [approx], 0, (0, 255, 0), 3)\n",
    "    \n",
    "    #cv2.imshow('Contour', img)\n",
    "    plt.imshow(img)\n",
    "    #cv2.imshow('Approx', img2)\n",
    "    #plt.imshow(img2)\n",
    "    \n",
    "    #cv2.waitKey(0)\n",
    "    #cv2.destroyAllWindows()\n",
    "    #cv2.waitKey(1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    contour_approx() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# OpenCV - 투영변환 구현하기 (1)\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def warp_affine():\n",
    "    img = cv2.imread('images/transform.png')\n",
    "    \n",
    "    #x , y\n",
    "    pts1 = np.float32([[50, 50], [200, 50], [20, 200]])#옮기기 전 좌표 \n",
    "    pts2 = np.float32([[70, 100], [220, 50], [150, 250]])#옮긴 후 좌표\n",
    "    \n",
    "    M = cv2.getAffineTransform(pts1, pts2)#해당 좌표만 옮겨지는게 아닌 그 주변의 좌표도 옮겨지는 매트릭스 변환\n",
    "    \n",
    "    result = cv2.warpAffine(img, M, (350, 300))#이미지, 매트릭스, 창 크기\n",
    "    \n",
    "    #cv2.imshow('original', img)\n",
    "    plt.imshow(img)\n",
    "    #cv2.imshow('Affine Transform', result)\n",
    "    plt.imshow(result)\n",
    "    \n",
    "    #cv2.waitKey(0)\n",
    "    #cv2.destroyAllWindows()\n",
    "    #cv2.waitKey(1)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    warp_affine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# OpenCV - 투영변환 구현하기 (2)\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def warp_perspective():\n",
    "    img = cv2.imread('images/transform.jpg')\n",
    "    \n",
    "    #외곽 추출 알고리즘을 돌려 4개의 꼭지점을 인식했다는가정하에\n",
    "    topLeft = [127, 157]\n",
    "    topRight = [448, 152]\n",
    "    bottomRight = [579, 526]\n",
    "    bottomLeft = [54, 549]\n",
    "    \n",
    "    pts1 = np.float32([topLeft, topRight, bottomRight, bottomLeft])\n",
    "    \n",
    "    w1 = abs(bottomRight[0] - bottomLeft[0])\n",
    "    w2 = abs(topRight[0] - topLeft[0])\n",
    "    h1 = abs(topRight[1] - bottomRight[1])\n",
    "    h2 = abs(topLeft[1] - bottomLeft[1])\n",
    "    minWidth = min([w1, w2])\n",
    "    minHeight = min([h1, h2])\n",
    "    \n",
    "    pts2 = np.float32([[0,0], [minWidth-1,0], \n",
    "                      [minWidth-1,minHeight-1], [0,minHeight-1]])\n",
    "    \n",
    "    M = cv2.getPerspectiveTransform(pts1, pts2)#원근법 보정 함수\n",
    "    \n",
    "    result = cv2.warpPerspective(img, M, (int(minWidth), int(minHeight)))\n",
    "    \n",
    "    #cv2.imshow('original', img)\n",
    "    plt.imshow(img)\n",
    "    #cv2.imshow('Warp Transform', result)\n",
    "    plt.imshow(result)\n",
    "    \n",
    "    #cv2.waitKey(0)\n",
    "    #cv2.destroyAllWindows()\n",
    "    #cv2.waitKey(1)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    warp_perspective()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# OpenCV - 스캔한 듯한 효과 주기 (1)\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Callback Function for Trackbar (but do not any work)\n",
    "def nothing(x):\n",
    "    pass\n",
    "\n",
    "def global_threshold():\n",
    "    imgfile = 'images/document.jpg'\n",
    "    img = cv2.imread(imgfile, cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    # Resize image\n",
    "    r = 600.0 / img.shape[0]\n",
    "    dim = (int(img.shape[1] * r), 600)#비율유지 하면서 가로 길이 600\n",
    "    img = cv2.resize(img, dim, interpolation = cv2.INTER_AREA)\n",
    "    \n",
    "    WindowName = \"Window\"\n",
    "    TrackbarName = \"Threshold\"\n",
    "    \n",
    "    # Make Window and Trackbar\n",
    "    #cv2.namedWindow(WindowName)\n",
    "    #cv2.createTrackbar(TrackbarName, WindowName, 70, 255, nothing)\n",
    "    \n",
    "    # Allocate destination image\n",
    "    Threshold = np.zeros(img.shape, np.uint8)\n",
    "    \n",
    "    # Loop for get trackbar pos and process it\n",
    "    #while True:\n",
    "    #    # Get position in trackbar\n",
    "    #    TrackbarPos = cv2.getTrackbarPos(TrackbarName, WindowName)\n",
    "    #    # Apply threshold\n",
    "    #    cv2.threshold(img, TrackbarPos, 255, cv2.THRESH_BINARY, Threshold)\n",
    "    #    # Show in window\n",
    "    #    cv2.imshow(WindowName, Threshold)\n",
    "    #    \n",
    "    #    # wait for ESC key to exit\n",
    "    #    k = cv2.waitKey(0)\n",
    "    #    if k == 27:\n",
    "    #        cv2.destroyAllWindows()\n",
    "    #        cv2.waitKey(1)\n",
    "    #        break\n",
    "    cv2.threshold(img, 200, 255, cv2.THRESH_BINARY, Threshold)#타겟 이미지, 쓰레시 홀드 값, ?, , 결과 이미지\n",
    "    plt.imshow(Threshold, cmap = \"gray\")\n",
    "    return\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    global_threshold() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# OpenCV - 스캔한 듯한 효과 주기 (2)\n",
    "# 이미지를 잘게 쪼갠 후 쓰레시 홀드 주기 알고리즘 주변 밝기 평균 - 상수\n",
    "# 블러효과를 줌으로써 외각좌표를 추출하고 원본이미지에 그 좌표를 사용하여 글자 인식하는데 쓰이면 됨\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def adaptive_threshold():\n",
    "    imgfile = 'images/document.jpg'\n",
    "    img = cv2.imread(imgfile, cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    # Resize image\n",
    "    r = 600.0 / img.shape[0]\n",
    "    dim = (int(img.shape[1] * r), 600)\n",
    "    img = cv2.resize(img, dim, interpolation = cv2.INTER_AREA)\n",
    "    \n",
    "    # Blur image and apply adaptive threshold\n",
    "    blur = cv2.GaussianBlur(img, (5, 5), 0)#주변 픽셀 크기 \n",
    "    result_without_blur = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 21, 10)\n",
    "    #타겟 이미지, 쓰레쉬 홀드 최대값, 알고리즘 이름, '', 쪼개는 정도<튜토리얼 권장 값>, 뺄 상수 값<튜토리얼 권장 값>\n",
    "    result_with_blur = cv2.adaptiveThreshold(blur, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 21, 10)\n",
    "    #cv2.imshow('Without Blur', result_without_blur)\n",
    "    plt.imshow(result_without_blur, cmap = \"gray\")\n",
    "    #cv2.imshow('With Blur', result_with_blur)\n",
    "    plt.imshow(result_with_blur, cmap = \"gray\")\n",
    "    \n",
    "    #cv2.waitKey(0)\n",
    "    #cv2.destroyAllWindows()\n",
    "    #cv2.waitKey(1)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    adaptive_threshold() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 명함인식 구현하기 - 캡처된 이미지\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def order_points(pts):\n",
    "    # initialzie a list of coordinates that will be ordered\n",
    "    # such that the first entry in the list is the top-left,\n",
    "    # the second entry is the top-right, the third is the\n",
    "    # bottom-right, and the fourth is the bottom-left\n",
    "    rect = np.zeros((4, 2), dtype = \"float32\")\n",
    "\n",
    "    # the top-left point will have the smallest sum, whereas\n",
    "    # the bottom-right point will have the largest sum\n",
    "    s = pts.sum(axis = 1)\n",
    "    rect[0] = pts[np.argmin(s)]\n",
    "    rect[2] = pts[np.argmax(s)]\n",
    "\n",
    "    # now, compute the difference between the points, the\n",
    "    # top-right point will have the smallest difference,\n",
    "    # whereas the bottom-left will have the largest difference\n",
    "    diff = np.diff(pts, axis = 1)\n",
    "    rect[1] = pts[np.argmin(diff)]\n",
    "    rect[3] = pts[np.argmax(diff)]\n",
    "\n",
    "    # return the ordered coordinates\n",
    "    return rect\n",
    "\n",
    "def auto_scan_image():\n",
    "    # load the image and compute the ratio of the old height\n",
    "    # to the new height, clone it, and resize it\n",
    "    # document.jpg ~ docuemnt7.jpg\n",
    "    image = cv2.imread('images/document.jpg')\n",
    "    orig = image.copy()\n",
    "    r = 800.0 / image.shape[0]\n",
    "    dim = (int(image.shape[1] * r), 800)\n",
    "    image = cv2.resize(image, dim, interpolation = cv2.INTER_AREA)\n",
    "\n",
    "    # convert the image to grayscale, blur it, and find edges\n",
    "    # in the image\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    gray = cv2.GaussianBlur(gray, (3, 3), 0)\n",
    "    edged = cv2.Canny(gray, 75, 200)\n",
    "\n",
    "    # show the original image and the edge detected image\n",
    "    print \"STEP 1: Edge Detection\"\n",
    "    plt.imshow(image, cmap = \"gray\")\n",
    "    plt.imshow(edged, cmap = \"gray\")\n",
    "    #cv2.imshow(\"Image\", image)\n",
    "    #cv2.imshow(\"Edged\", edged)\n",
    "    \n",
    "    #cv2.waitKey(0)\n",
    "    #cv2.destroyAllWindows()\n",
    "    #cv2.waitKey(1)\n",
    "\n",
    "    # find the contours in the edged image, keeping only the\n",
    "    # largest ones, and initialize the screen contour\n",
    "    (_, cnts, _) = cv2.findContours(edged.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cnts = sorted(cnts, key = cv2.contourArea, reverse = True)[:5]#면적이 큰 순서대로 5개만 정렬\n",
    "\n",
    "    # loop over the contours\n",
    "    for c in cnts:\n",
    "        # approximate the contour\n",
    "        peri = cv2.arcLength(c, True)#오차 구함\n",
    "        approx = cv2.approxPolyDP(c, 0.02 * peri, True)#근사 하여 외곽선 추출\n",
    "\n",
    "        # if our approximated contour has four points, then we\n",
    "        # can assume that we have found our screen\n",
    "        if len(approx) == 4:#꼭지점 4개 발견시 종료\n",
    "            screenCnt = approx\n",
    "            break\n",
    "\n",
    "    # show the contour (outline) of the piece of paper\n",
    "    print \"STEP 2: Find contours of paper\"\n",
    "    cv2.drawContours(image, [screenCnt], -1, (0, 255, 0), 2)\n",
    "    plt.imshow(image, cmap = \"gray\")\n",
    "    #cv2.imshow(\"Outline\", image)\n",
    "    \n",
    "    #cv2.waitKey(0)\n",
    "    #cv2.destroyAllWindows()\n",
    "    #cv2.waitKey(1)\n",
    "\n",
    "    # apply the four point transform to obtain a top-down\n",
    "    # view of the original image\n",
    "    rect = order_points(screenCnt.reshape(4, 2) / r)#사각형의 꼭지점을 추출해내는거 \n",
    "    #왼쪽 위는 x+y가 가장 작은곳\n",
    "    #오른쪽 위는 y-x가 가장 작은곳\n",
    "    #왼쪽 아래는 ???\n",
    "    #오른쪽 아래는 x+y가 가장 큰곳\n",
    "    (topLeft, topRight, bottomRight, bottomLeft) = rect\n",
    "    \n",
    "    w1 = abs(bottomRight[0] - bottomLeft[0])\n",
    "    w2 = abs(topRight[0] - topLeft[0])\n",
    "    h1 = abs(topRight[1] - bottomRight[1])\n",
    "    h2 = abs(topLeft[1] - bottomLeft[1])\n",
    "    maxWidth = max([w1, w2])\n",
    "    maxHeight = max([h1, h2])\n",
    "    \n",
    "    dst = np.float32([[0,0], [maxWidth-1,0], \n",
    "                      [maxWidth-1,maxHeight-1], [0,maxHeight-1]])\n",
    "    \n",
    "    M = cv2.getPerspectiveTransform(rect, dst)\n",
    "    warped = cv2.warpPerspective(orig, M, (maxWidth, maxHeight))\n",
    "\n",
    "    # show the original and scanned images\n",
    "    print \"STEP 3: Apply perspective transform\"\n",
    "    plt.imshow(warped, cmap = \"gray\")\n",
    "    #cv2.imshow(\"Warped\", warped)\n",
    "    \n",
    "    #cv2.waitKey(0)\n",
    "    #cv2.destroyAllWindows()\n",
    "    #cv2.waitKey(1)\n",
    "    \n",
    "    # convert the warped image to grayscale, then threshold it\n",
    "    # to give it that 'black and white' paper effect\n",
    "    warped = cv2.cvtColor(warped, cv2.COLOR_BGR2GRAY)\n",
    "    warped = cv2.adaptiveThreshold(warped, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 21, 10)\n",
    "\n",
    "    # show the original and scanned images\n",
    "    print \"STEP 4: Apply Adaptive Threshold\"\n",
    "    plt.imshow(orig, cmap = \"gray\")\n",
    "    plt.imshow(warped, cmap = \"gray\")\n",
    "    #cv2.imshow(\"Original\", orig)\n",
    "    #cv2.imshow(\"Scanned\", warped)\n",
    "    #cv2.imwrite('scannedImage.png', warped)\n",
    "    \n",
    "    #cv2.waitKey(0)\n",
    "    #cv2.destroyAllWindows()\n",
    "    #cv2.waitKey(1)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    auto_scan_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 명함인식 구현하기 - 웹캠(1)\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def auto_scan_image_via_webcam():\n",
    "    \n",
    "    try: \n",
    "        cap = cv2.VideoCapture(0)\n",
    "    except:\n",
    "        print 'cannot load camera!'\n",
    "        return\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print 'cannot load camera!'\n",
    "            break\n",
    "            \n",
    "        k = cv2.waitKey(10)\n",
    "        if k == 27:\n",
    "            break\n",
    "        \n",
    "        # convert the image to grayscale, blur it, and find edges\n",
    "        # in the image\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        gray = cv2.GaussianBlur(gray, (3, 3), 0)\n",
    "        edged = cv2.Canny(gray, 75, 200)\n",
    "\n",
    "        # show the original image and the edge detected image\n",
    "        print \"STEP 1: Edge Detection\"\n",
    "\n",
    "        # find the contours in the edged image, keeping only the\n",
    "        # largest ones, and initialize the screen contour\n",
    "        (_, cnts, _) = cv2.findContours(edged.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        cnts = sorted(cnts, key = cv2.contourArea, reverse = True)[:5]\n",
    "\n",
    "        # loop over the contours\n",
    "        for c in cnts:\n",
    "            # approximate the contour\n",
    "            peri = cv2.arcLength(c, True)\n",
    "            approx = cv2.approxPolyDP(c, 0.02 * peri, True)\n",
    "            screenCnt = []\n",
    "\n",
    "            # if our approximated contour has four points, then we\n",
    "            # can assume that we have found our screen\n",
    "            if len(approx) == 4:\n",
    "                contourSize = cv2.contourArea(approx)\n",
    "                camSize = frame.shape[0] * frame.shape[1]\n",
    "                ratio = contourSize / camSize\n",
    "                print contourSize\n",
    "                print camSize\n",
    "                print ratio\n",
    "                \n",
    "                if ratio > 0.1:\n",
    "                    screenCnt = approx\n",
    "                    \n",
    "                break \n",
    "        \n",
    "        if len(screenCnt) == 0:\n",
    "            cv2.imshow(\"WebCam\", frame)\n",
    "            continue\n",
    "            \n",
    "        else:\n",
    "            # show the contour (outline) of the piece of paper\n",
    "            print \"STEP 2: Find contours of paper\"\n",
    "\n",
    "            cv2.drawContours(frame, [screenCnt], -1, (0, 255, 0), 2)\n",
    "            cv2.imshow(\"WebCam\", frame)\n",
    "        \n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    auto_scan_image_via_webcam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 명함인식 구현하기 - 웹캠(2)\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def order_points(pts):\n",
    "    # initialzie a list of coordinates that will be ordered\n",
    "    # such that the first entry in the list is the top-left,\n",
    "    # the second entry is the top-right, the third is the\n",
    "    # bottom-right, and the fourth is the bottom-left\n",
    "    rect = np.zeros((4, 2), dtype = \"float32\")\n",
    "\n",
    "    # the top-left point will have the smallest sum, whereas\n",
    "    # the bottom-right point will have the largest sum\n",
    "    s = pts.sum(axis = 1)\n",
    "    rect[0] = pts[np.argmin(s)]\n",
    "    rect[2] = pts[np.argmax(s)]\n",
    "\n",
    "    # now, compute the difference between the points, the\n",
    "    # top-right point will have the smallest difference,\n",
    "    # whereas the bottom-left will have the largest difference\n",
    "    diff = np.diff(pts, axis = 1)\n",
    "    rect[1] = pts[np.argmin(diff)]\n",
    "    rect[3] = pts[np.argmax(diff)]\n",
    "\n",
    "    # return the ordered coordinates\n",
    "    return rect\n",
    "\n",
    "def auto_scan_image_via_webcam():\n",
    "    \n",
    "    try: \n",
    "        cap = cv2.VideoCapture(0)\n",
    "    except:\n",
    "        print 'cannot load camera!'\n",
    "        return\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print 'cannot load camera!'\n",
    "            break\n",
    "            \n",
    "        k = cv2.waitKey(10)\n",
    "        if k == 27:\n",
    "            break\n",
    "\n",
    "        # convert the image to grayscale, blur it, and find edges\n",
    "        # in the image\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        gray = cv2.GaussianBlur(gray, (3, 3), 0)\n",
    "        edged = cv2.Canny(gray, 75, 200)\n",
    "\n",
    "        # show the original image and the edge detected image\n",
    "        # print \"STEP 1: Edge Detection\"\n",
    "\n",
    "        # find the contours in the edged image, keeping only the\n",
    "        # largest ones, and initialize the screen contour\n",
    "        (cnts, _) = cv2.findContours(edged.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        cnts = sorted(cnts, key = cv2.contourArea, reverse = True)[:5]\n",
    "\n",
    "        # loop over the contours\n",
    "        for c in cnts:\n",
    "            # approximate the contour\n",
    "            peri = cv2.arcLength(c, True)\n",
    "            approx = cv2.approxPolyDP(c, 0.02 * peri, True)\n",
    "            screenCnt = []\n",
    "\n",
    "            # if our approximated contour has four points, then we\n",
    "            # can assume that we have found our screen\n",
    "            if len(approx) == 4:\n",
    "                contourSize = cv2.contourArea(approx)\n",
    "                camSize = frame.shape[0] * frame.shape[1]\n",
    "                ratio = contourSize / camSize\n",
    "                print contourSize\n",
    "                print camSize\n",
    "                print ratio\n",
    "                \n",
    "                if ratio > 0.1:\n",
    "                    screenCnt = approx\n",
    "                    \n",
    "                break \n",
    "        \n",
    "        if len(screenCnt) == 0:\n",
    "            cv2.imshow(\"WebCam\", frame)\n",
    "            continue\n",
    "            \n",
    "        else:\n",
    "            # show the contour (outline) of the piece of paper\n",
    "            print \"STEP 2: Find contours of paper\"\n",
    "\n",
    "            cv2.drawContours(frame, [screenCnt], -1, (0, 255, 0), 2)\n",
    "            cv2.imshow(\"WebCam\", frame)\n",
    "            \n",
    "            # apply the four point transform to obtain a top-down\n",
    "            # view of the original image\n",
    "            rect = order_points(screenCnt.reshape(4, 2))\n",
    "            (topLeft, topRight, bottomRight, bottomLeft) = rect\n",
    "\n",
    "            w1 = abs(bottomRight[0] - bottomLeft[0])\n",
    "            w2 = abs(topRight[0] - topLeft[0])\n",
    "            h1 = abs(topRight[1] - bottomRight[1])\n",
    "            h2 = abs(topLeft[1] - bottomLeft[1])\n",
    "            maxWidth = max([w1, w2])\n",
    "            maxHeight = max([h1, h2])\n",
    "\n",
    "            dst = np.float32([[0,0], [maxWidth-1,0], \n",
    "                              [maxWidth-1,maxHeight-1], [0,maxHeight-1]])\n",
    "\n",
    "            M = cv2.getPerspectiveTransform(rect, dst)\n",
    "            warped = cv2.warpPerspective(frame, M, (maxWidth, maxHeight))\n",
    "\n",
    "            # show the original and scanned images\n",
    "            print \"STEP 3: Apply perspective transform\"\n",
    "\n",
    "            # convert the warped image to grayscale, then threshold it\n",
    "            # to give it that 'black and white' paper effect\n",
    "            warped = cv2.cvtColor(warped, cv2.COLOR_BGR2GRAY)\n",
    "            warped = cv2.adaptiveThreshold(warped, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 21, 10)\n",
    "\n",
    "            # show the original and scanned images\n",
    "            print \"STEP 4: Apply Adaptive Threshold\"\n",
    "\n",
    "            break\n",
    "        \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)\n",
    "    \n",
    "    cv2.imshow(\"Scanned\", warped)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)\n",
    "        \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    auto_scan_image_via_webcam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# OCR - Tesseract\n",
    "\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def ocr_tesseract():\n",
    "    image_file = 'images/scannedImage.png'\n",
    "    im = Image.open(image_file)\n",
    "    text = pytesseract.image_to_string(im)#글자인식\n",
    "    im.show()\n",
    "\n",
    "    print text\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ocr_tesseract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# OCR - Project Oxford by MS\n",
    "\n",
    "from PIL import Image\n",
    "import httplib, urllib, base64, json\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def print_text(json_data):\n",
    "    result = json.loads(json_data)\n",
    "    for l in result['regions']:\n",
    "        for w in l['lines']:\n",
    "            line = []\n",
    "            for r in w['words']:\n",
    "                line.append(r['text'])\n",
    "            print ' '.join(line)\n",
    "    return\n",
    "\n",
    "def ocr_project_oxford(headers, params, data):\n",
    "    conn = httplib.HTTPSConnection('westus.api.cognitive.microsoft.com')\n",
    "    conn.request(\"POST\", \"/vision/v1.0/ocr?%s\" % params, data, headers=headers)\n",
    "    response = conn.getresponse()\n",
    "    data = response.read()\n",
    "    print data + \"\\n\"\n",
    "    print_text(data)\n",
    "    conn.close()\n",
    "    return\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    headers = {\n",
    "        # Request headers\n",
    "        'Content-Type': 'application/octet-stream',\n",
    "        'Ocp-Apim-Subscription-Key': '',#여기에 project oxford홈페이지에서 무료 라이선스 키를 넣기\n",
    "    }\n",
    "    params = urllib.urlencode({\n",
    "        # Request parameters\n",
    "        'language': 'ko',#unk (unkown)\n",
    "        'detectOrientation ': 'true',\n",
    "    })\n",
    "    data = open('images/scannedImage.png', 'rb').read()\n",
    "    \n",
    "    try:\n",
    "        image_file = 'images/scannedImage.png'\n",
    "        im = Image.open(image_file)\n",
    "        im.show()\n",
    "        ocr_project_oxford(headers, params, data)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 명함인식 구현하기 - 웹캠 + OCR\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import httplib, urllib, base64, json\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def print_text(json_data):\n",
    "    result = json.loads(json_data)\n",
    "    for l in result['regions']:\n",
    "        for w in l['lines']:\n",
    "            line = []\n",
    "            for r in w['words']:\n",
    "                line.append(r['text'])\n",
    "            print ' '.join(line)\n",
    "    return\n",
    "\n",
    "def ocr_project_oxford(headers, params, data):\n",
    "    conn = httplib.HTTPSConnection('westus.api.cognitive.microsoft.com')\n",
    "    conn.request(\"POST\", \"/vision/v1.0/ocr?%s\" % params, data, headers=headers)\n",
    "    response = conn.getresponse()\n",
    "    data = response.read()\n",
    "    print data + \"\\n\"\n",
    "    print_text(data)\n",
    "    conn.close()\n",
    "    return\n",
    "\n",
    "def order_points(pts):\n",
    "    # initialzie a list of coordinates that will be ordered\n",
    "    # such that the first entry in the list is the top-left,\n",
    "    # the second entry is the top-right, the third is the\n",
    "    # bottom-right, and the fourth is the bottom-left\n",
    "    rect = np.zeros((4, 2), dtype = \"float32\")\n",
    "\n",
    "    # the top-left point will have the smallest sum, whereas\n",
    "    # the bottom-right point will have the largest sum\n",
    "    s = pts.sum(axis = 1)\n",
    "    rect[0] = pts[np.argmin(s)]\n",
    "    rect[2] = pts[np.argmax(s)]\n",
    "\n",
    "    # now, compute the difference between the points, the\n",
    "    # top-right point will have the smallest difference,\n",
    "    # whereas the bottom-left will have the largest difference\n",
    "    diff = np.diff(pts, axis = 1)\n",
    "    rect[1] = pts[np.argmin(diff)]\n",
    "    rect[3] = pts[np.argmax(diff)]\n",
    "\n",
    "    # return the ordered coordinates\n",
    "    return rect\n",
    "\n",
    "def auto_scan_image_via_webcam():\n",
    "    \n",
    "    try: \n",
    "        cap = cv2.VideoCapture(0)\n",
    "    except:\n",
    "        print 'cannot load camera!'\n",
    "        return\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print 'cannot load camera!'\n",
    "            break\n",
    "            \n",
    "        k = cv2.waitKey(10)\n",
    "        if k == 27:\n",
    "            break\n",
    "\n",
    "        # convert the image to grayscale, blur it, and find edges\n",
    "        # in the image\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        gray = cv2.GaussianBlur(gray, (3, 3), 0)\n",
    "        edged = cv2.Canny(gray, 75, 200)\n",
    "\n",
    "        # show the original image and the edge detected image\n",
    "        # print \"STEP 1: Edge Detection\"\n",
    "\n",
    "        # find the contours in the edged image, keeping only the\n",
    "        # largest ones, and initialize the screen contour\n",
    "        (_, cnts, _) = cv2.findContours(edged.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        cnts = sorted(cnts, key = cv2.contourArea, reverse = True)[:5]\n",
    "\n",
    "        # loop over the contours\n",
    "        for c in cnts:\n",
    "            # approximate the contour\n",
    "            peri = cv2.arcLength(c, True)\n",
    "            approx = cv2.approxPolyDP(c, 0.02 * peri, True)\n",
    "            screenCnt = []\n",
    "\n",
    "            # if our approximated contour has four points, then we\n",
    "            # can assume that we have found our screen\n",
    "            if len(approx) == 4:\n",
    "                contourSize = cv2.contourArea(approx)\n",
    "                camSize = frame.shape[0] * frame.shape[1]\n",
    "                ratio = contourSize / camSize\n",
    "                # print contourSize\n",
    "                # print camSize\n",
    "                # print ratio\n",
    "                \n",
    "                if ratio > 0.1:\n",
    "                    screenCnt = approx\n",
    "                    \n",
    "                break \n",
    "        \n",
    "        if len(screenCnt) == 0:\n",
    "            cv2.imshow(\"WebCam\", frame)\n",
    "            continue\n",
    "            \n",
    "        else:\n",
    "            # show the contour (outline) of the piece of paper\n",
    "            print \"STEP 2: Find contours of paper\"\n",
    "\n",
    "            cv2.drawContours(frame, [screenCnt], -1, (0, 255, 0), 2)\n",
    "            cv2.imshow(\"WebCam\", frame)\n",
    "            \n",
    "            # apply the four point transform to obtain a top-down\n",
    "            # view of the original image\n",
    "            rect = order_points(screenCnt.reshape(4, 2))\n",
    "            (topLeft, topRight, bottomRight, bottomLeft) = rect\n",
    "\n",
    "            w1 = abs(bottomRight[0] - bottomLeft[0])\n",
    "            w2 = abs(topRight[0] - topLeft[0])\n",
    "            h1 = abs(topRight[1] - bottomRight[1])\n",
    "            h2 = abs(topLeft[1] - bottomLeft[1])\n",
    "            maxWidth = max([w1, w2])\n",
    "            maxHeight = max([h1, h2])\n",
    "\n",
    "            dst = np.float32([[0,0], [maxWidth-1,0], \n",
    "                              [maxWidth-1,maxHeight-1], [0,maxHeight-1]])\n",
    "\n",
    "            M = cv2.getPerspectiveTransform(rect, dst)\n",
    "            warped = cv2.warpPerspective(frame, M, (maxWidth, maxHeight))\n",
    "\n",
    "            # show the original and scanned images\n",
    "            print \"STEP 3: Apply perspective transform\"\n",
    "\n",
    "            # convert the warped image to grayscale, then threshold it\n",
    "            # to give it that 'black and white' paper effect\n",
    "            warped = cv2.cvtColor(warped, cv2.COLOR_BGR2GRAY)\n",
    "            warped = cv2.adaptiveThreshold(warped, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 21, 10)\n",
    "\n",
    "            # show the original and scanned images\n",
    "            print \"STEP 4: Apply Adaptive Threshold\"\n",
    "\n",
    "            break\n",
    "        \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)\n",
    "    \n",
    "    cv2.imshow(\"Scanned\", warped)\n",
    "    cv2.imwrite('scannedImage2.png', warped)\n",
    "    \n",
    "    headers = {\n",
    "        # Request headers\n",
    "        'Content-Type': 'application/octet-stream',\n",
    "        'Ocp-Apim-Subscription-Key': '',\n",
    "    }\n",
    "    params = urllib.urlencode({\n",
    "        # Request parameters\n",
    "        'language': 'unk',\n",
    "        'detectOrientation ': 'true',\n",
    "    })\n",
    "    data = open('scannedImage2.png', 'rb').read()\n",
    "    \n",
    "    try:\n",
    "        image_file = 'scannedImage2.png'\n",
    "        ocr_project_oxford(headers, params, data)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    \n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)\n",
    "        \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    auto_scan_image_via_webcam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (참고) OpenCV - 이미지에서 텍스트 영역만 찾아내기\n",
    "\n",
    "# 출처: http://www.danvk.org/2015/01/07/finding-blocks-of-text-in-an-image-using-python-opencv-and-numpy.html\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "import cv2\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "from scipy.ndimage.filters import rank_filter\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "def dilate(ary, N, iterations): \n",
    "    \"\"\"Dilate using an NxN '+' sign shape. ary is np.uint8.\"\"\"\n",
    "    kernel = np.zeros((N,N), dtype=np.uint8)\n",
    "    kernel[(N-1)/2,:] = 1\n",
    "    dilated_image = cv2.dilate(ary / 255, kernel, iterations=iterations)\n",
    "\n",
    "    kernel = np.zeros((N,N), dtype=np.uint8)\n",
    "    kernel[:,(N-1)/2] = 1\n",
    "    dilated_image = cv2.dilate(dilated_image, kernel, iterations=iterations)\n",
    "    return dilated_image\n",
    "\n",
    "\n",
    "def props_for_contours(contours, ary):\n",
    "    \"\"\"Calculate bounding box & the number of set pixels for each contour.\"\"\"\n",
    "    c_info = []\n",
    "    for c in contours:\n",
    "        x,y,w,h = cv2.boundingRect(c)\n",
    "        c_im = np.zeros(ary.shape)\n",
    "        cv2.drawContours(c_im, [c], 0, 255, -1)\n",
    "        c_info.append({\n",
    "            'x1': x,\n",
    "            'y1': y,\n",
    "            'x2': x + w - 1,\n",
    "            'y2': y + h - 1,\n",
    "            'sum': np.sum(ary * (c_im > 0))/255\n",
    "        })\n",
    "    return c_info\n",
    "\n",
    "\n",
    "def union_crops(crop1, crop2):\n",
    "    \"\"\"Union two (x1, y1, x2, y2) rects.\"\"\"\n",
    "    x11, y11, x21, y21 = crop1\n",
    "    x12, y12, x22, y22 = crop2\n",
    "    return min(x11, x12), min(y11, y12), max(x21, x22), max(y21, y22)\n",
    "\n",
    "\n",
    "def intersect_crops(crop1, crop2):\n",
    "    x11, y11, x21, y21 = crop1\n",
    "    x12, y12, x22, y22 = crop2\n",
    "    return max(x11, x12), max(y11, y12), min(x21, x22), min(y21, y22)\n",
    "\n",
    "\n",
    "def crop_area(crop):\n",
    "    x1, y1, x2, y2 = crop\n",
    "    return max(0, x2 - x1) * max(0, y2 - y1)\n",
    "\n",
    "\n",
    "def find_border_components(contours, ary):\n",
    "    borders = []\n",
    "    area = ary.shape[0] * ary.shape[1]\n",
    "    for i, c in enumerate(contours):\n",
    "        x,y,w,h = cv2.boundingRect(c)\n",
    "        if w * h > 0.5 * area:\n",
    "            borders.append((i, x, y, x + w - 1, y + h - 1))\n",
    "    return borders\n",
    "\n",
    "\n",
    "def angle_from_right(deg):\n",
    "    return min(deg % 90, 90 - (deg % 90))\n",
    "\n",
    "\n",
    "def remove_border(contour, ary):\n",
    "    \"\"\"Remove everything outside a border contour.\"\"\"\n",
    "    # Use a rotated rectangle (should be a good approximation of a border).\n",
    "    # If it's far from a right angle, it's probably two sides of a border and\n",
    "    # we should use the bounding box instead.\n",
    "    c_im = np.zeros(ary.shape)\n",
    "    r = cv2.minAreaRect(contour)\n",
    "    degs = r[2]\n",
    "    if angle_from_right(degs) <= 10.0:\n",
    "        box = cv2.cv.BoxPoints(r)\n",
    "        box = np.int0(box)\n",
    "        cv2.drawContours(c_im, [box], 0, 255, -1)\n",
    "        cv2.drawContours(c_im, [box], 0, 0, 4)\n",
    "    else:\n",
    "        x1, y1, x2, y2 = cv2.boundingRect(contour)\n",
    "        cv2.rectangle(c_im, (x1, y1), (x2, y2), 255, -1)\n",
    "        cv2.rectangle(c_im, (x1, y1), (x2, y2), 0, 4)\n",
    "\n",
    "    return np.minimum(c_im, ary)\n",
    "\n",
    "\n",
    "def find_components(edges, max_components=16):\n",
    "    \"\"\"Dilate the image until there are just a few connected components.\n",
    "    Returns contours for these components.\"\"\"\n",
    "    # Perform increasingly aggressive dilation until there are just a few\n",
    "    # connected components.\n",
    "    count = 21\n",
    "    dilation = 5\n",
    "    n = 1\n",
    "    while count > 16:\n",
    "        n += 1\n",
    "        dilated_image = dilate(edges, N=3, iterations=n)\n",
    "        _, contours, hierarchy = cv2.findContours(dilated_image, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        count = len(contours)\n",
    "    # print dilation\n",
    "    # Image.fromarray(edges).show()\n",
    "    # Image.fromarray(255 * dilated_image).show()\n",
    "    return contours\n",
    "\n",
    "\n",
    "def find_optimal_components_subset(contours, edges):\n",
    "    \"\"\"Find a crop which strikes a good balance of coverage/compactness.\n",
    "    Returns an (x1, y1, x2, y2) tuple.\n",
    "    \"\"\"\n",
    "    c_info = props_for_contours(contours, edges)\n",
    "    c_info.sort(key=lambda x: -x['sum'])\n",
    "    total = np.sum(edges) / 255\n",
    "    area = edges.shape[0] * edges.shape[1]\n",
    "\n",
    "    c = c_info[0]\n",
    "    del c_info[0]\n",
    "    this_crop = c['x1'], c['y1'], c['x2'], c['y2']\n",
    "    crop = this_crop\n",
    "    covered_sum = c['sum']\n",
    "\n",
    "    while covered_sum < total:\n",
    "        changed = False\n",
    "        recall = 1.0 * covered_sum / total\n",
    "        prec = 1 - 1.0 * crop_area(crop) / area\n",
    "        f1 = 2 * (prec * recall / (prec + recall))\n",
    "        #print '----'\n",
    "        for i, c in enumerate(c_info):\n",
    "            this_crop = c['x1'], c['y1'], c['x2'], c['y2']\n",
    "            new_crop = union_crops(crop, this_crop)\n",
    "            new_sum = covered_sum + c['sum']\n",
    "            new_recall = 1.0 * new_sum / total\n",
    "            new_prec = 1 - 1.0 * crop_area(new_crop) / area\n",
    "            new_f1 = 2 * new_prec * new_recall / (new_prec + new_recall)\n",
    "\n",
    "            # Add this crop if it improves f1 score,\n",
    "            # _or_ it adds 25% of the remaining pixels for <15% crop expansion.\n",
    "            # ^^^ very ad-hoc! make this smoother\n",
    "            remaining_frac = c['sum'] / (total - covered_sum)\n",
    "            new_area_frac = 1.0 * crop_area(new_crop) / crop_area(crop) - 1\n",
    "            if new_f1 > f1 or (\n",
    "                    remaining_frac > 0.25 and new_area_frac < 0.15):\n",
    "                print '%d %s -> %s / %s (%s), %s -> %s / %s (%s), %s -> %s' % (\n",
    "                        i, covered_sum, new_sum, total, remaining_frac,\n",
    "                        crop_area(crop), crop_area(new_crop), area, new_area_frac,\n",
    "                        f1, new_f1)\n",
    "                crop = new_crop\n",
    "                covered_sum = new_sum\n",
    "                del c_info[i]\n",
    "                changed = True\n",
    "                break\n",
    "\n",
    "        if not changed:\n",
    "            break\n",
    "\n",
    "    return crop\n",
    "\n",
    "\n",
    "def pad_crop(crop, contours, edges, border_contour, pad_px=15):\n",
    "    \"\"\"Slightly expand the crop to get full contours.\n",
    "    This will expand to include any contours it currently intersects, but will\n",
    "    not expand past a border.\n",
    "    \"\"\"\n",
    "    bx1, by1, bx2, by2 = 0, 0, edges.shape[0], edges.shape[1]\n",
    "    if border_contour is not None and len(border_contour) > 0:\n",
    "        c = props_for_contours([border_contour], edges)[0]\n",
    "        bx1, by1, bx2, by2 = c['x1'] + 5, c['y1'] + 5, c['x2'] - 5, c['y2'] - 5\n",
    "\n",
    "    def crop_in_border(crop):\n",
    "        x1, y1, x2, y2 = crop\n",
    "        x1 = max(x1 - pad_px, bx1)\n",
    "        y1 = max(y1 - pad_px, by1)\n",
    "        x2 = min(x2 + pad_px, bx2)\n",
    "        y2 = min(y2 + pad_px, by2)\n",
    "        return crop\n",
    "    \n",
    "    crop = crop_in_border(crop)\n",
    "\n",
    "    c_info = props_for_contours(contours, edges)\n",
    "    changed = False\n",
    "    for c in c_info:\n",
    "        this_crop = c['x1'], c['y1'], c['x2'], c['y2']\n",
    "        this_area = crop_area(this_crop)\n",
    "        int_area = crop_area(intersect_crops(crop, this_crop))\n",
    "        new_crop = crop_in_border(union_crops(crop, this_crop))\n",
    "        if 0 < int_area < this_area and crop != new_crop:\n",
    "            print '%s -> %s' % (str(crop), str(new_crop))\n",
    "            changed = True\n",
    "            crop = new_crop\n",
    "\n",
    "    if changed:\n",
    "        return pad_crop(crop, contours, edges, border_contour, pad_px)\n",
    "    else:\n",
    "        return crop\n",
    "\n",
    "\n",
    "def downscale_image(im, max_dim=2048):\n",
    "    \"\"\"Shrink im until its longest dimension is <= max_dim.\n",
    "    Returns new_image, scale (where scale <= 1).\n",
    "    \"\"\"\n",
    "    a, b = im.size\n",
    "    if max(a, b) <= max_dim:\n",
    "        return 1.0, im\n",
    "\n",
    "    scale = 1.0 * max_dim / max(a, b)\n",
    "    new_im = im.resize((int(a * scale), int(b * scale)), Image.ANTIALIAS)\n",
    "    return scale, new_im\n",
    "\n",
    "\n",
    "def process_image(path, out_path):\n",
    "    orig_im = Image.open(path)\n",
    "    scale, im = downscale_image(orig_im)\n",
    "\n",
    "    edges = cv2.Canny(np.asarray(im), 100, 200)\n",
    "\n",
    "    # TODO: dilate image _before_ finding a border. This is crazy sensitive!\n",
    "    _, contours, hierarchy = cv2.findContours(edges, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    borders = find_border_components(contours, edges)\n",
    "    borders.sort(key=lambda (i, x1, y1, x2, y2): (x2 - x1) * (y2 - y1))\n",
    "\n",
    "    border_contour = None\n",
    "    if len(borders):\n",
    "        border_contour = contours[borders[0][0]]\n",
    "        edges = remove_border(border_contour, edges)\n",
    "\n",
    "    edges = 255 * (edges > 0).astype(np.uint8)\n",
    "\n",
    "    # Remove ~1px borders using a rank filter.\n",
    "    maxed_rows = rank_filter(edges, -5, size=(1, 20))\n",
    "    maxed_cols = rank_filter(edges, -5, size=(20, 1))\n",
    "    debordered = np.minimum(np.minimum(edges, maxed_rows), maxed_cols)\n",
    "    edges = debordered\n",
    "\n",
    "    contours = find_components(edges)\n",
    "    if len(contours) == 0:\n",
    "        print '%s -> (no text!)' % path\n",
    "        return\n",
    "\n",
    "    crop = find_optimal_components_subset(contours, edges)\n",
    "    crop = pad_crop(crop, contours, edges, border_contour)\n",
    "\n",
    "    crop = [int(x / scale) for x in crop]  # upscale to the original image size.\n",
    "    \n",
    "    # draw and show cropped rectangle area in the original image\n",
    "    rgb_im = im.convert('RGB')\n",
    "    draw = ImageDraw.Draw(rgb_im)\n",
    "    draw.rectangle(crop, outline='red')\n",
    "    rgb_im.show()\n",
    "    \n",
    "    text_im = orig_im.crop(crop)\n",
    "    text_im.show()\n",
    "    text_im.save(out_path)\n",
    "    print '%s -> %s' % (path, out_path)\n",
    "\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    # path = 'images/text.jpg'\n",
    "    path = 'images/scannedImage.png'\n",
    "    out_path = 'croppedImage.png'\n",
    "    try:\n",
    "        process_image(path, out_path)\n",
    "    except Exception as e:\n",
    "        print '%s %s' % (path, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
